{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e21e38c",
   "metadata": {},
   "source": [
    "## Apache Airflow (OPTIONAL) \n",
    "\n",
    "In this section of the hands-on-lab, we will utilize Snowpark's Python client-side Dataframe API as well as the Snowpark server-side runtime and Apache Airflow to create an operational pipeline.  We will take the functions created by the ML Ops team and create a directed acyclic graph (DAG) of operations to run each month when new data is available. \n",
    "\n",
    "Note: This code requires the ability to run docker containers locally.  If you do not have Docker Desktop you can run the same pipeline from a python kernel via the 04_ML_Ops.ipynb notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c266ce3",
   "metadata": {},
   "source": [
    "We will use the dev CLI from Astronomer. https://docs.astronomer.io/astro/cli/get-started#step-1-install-the-astro-cli\n",
    "\n",
    "Follow the instructions to install the `astro` CLI for your particular local setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ad3fbdcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting dags/airflow_tasks.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile dags/airflow_tasks.py\n",
    "\n",
    "from airflow.decorators import task\n",
    "\n",
    "@task.virtualenv(python_version=3.8)\n",
    "def snowpark_database_setup(state_dict:dict)-> dict: \n",
    "    import snowflake.snowpark.functions as F\n",
    "    from dags.snowpark_connection import snowpark_connect\n",
    "    from dags.elt import reset_database\n",
    "\n",
    "    session, _ = snowpark_connect('./include/state.json')\n",
    "    reset_database(session=session, state_dict=state_dict, prestaged=True)\n",
    "\n",
    "    _ = session.sql('CREATE STAGE '+state_dict['model_stage_name']).collect()\n",
    "    _ = session.sql('CREATE TAG model_id_tag').collect()\n",
    "\n",
    "    session.close()\n",
    "\n",
    "    return state_dict\n",
    "\n",
    "@task.virtualenv(python_version=3.8)\n",
    "def incremental_elt_task(state_dict: dict, files_to_download:list)-> dict:\n",
    "    from dags.ingest import incremental_elt\n",
    "    from dags.snowpark_connection import snowpark_connect\n",
    "\n",
    "    session, _ = snowpark_connect()\n",
    "\n",
    "    print('Ingesting '+str(files_to_download))\n",
    "    download_base_url=state_dict['connection_parameters']['download_base_url']\n",
    "\n",
    "    _ = session.use_warehouse(state_dict['compute_parameters']['load_warehouse'])\n",
    "\n",
    "    _ = incremental_elt(session=session, \n",
    "                        state_dict=state_dict, \n",
    "                        files_to_ingest=files_to_download,\n",
    "                        download_base_url=download_base_url,\n",
    "                        use_prestaged=True)\n",
    "\n",
    "    #_ = session.sql('ALTER WAREHOUSE IF EXISTS '+state_dict['compute_parameters']['load_warehouse']+\\\n",
    "    #                ' SUSPEND').collect()\n",
    "\n",
    "    session.close()\n",
    "    return state_dict\n",
    "\n",
    "@task.virtualenv(python_version=3.8)\n",
    "def initial_bulk_load_task(state_dict:dict)-> dict:\n",
    "    from dags.ingest import bulk_elt\n",
    "    from dags.snowpark_connection import snowpark_connect\n",
    "\n",
    "    session, _ = snowpark_connect()\n",
    "\n",
    "    _ = session.use_warehouse(state_dict['compute_parameters']['load_warehouse'])\n",
    "\n",
    "    print('Running initial bulk ingest from '+state_dict['connection_parameters']['download_base_url'])\n",
    "    \n",
    "    _ = bulk_elt(session=session, \n",
    "                 state_dict=state_dict, \n",
    "                 download_base_url=state_dict['connection_parameters']['download_base_url'],\n",
    "                 use_prestaged=True)\n",
    "\n",
    "    #_ = session.sql('ALTER WAREHOUSE IF EXISTS '+state_dict['compute_parameters']['load_warehouse']+\\\n",
    "    #                ' SUSPEND').collect()\n",
    "\n",
    "    session.close()\n",
    "    return state_dict\n",
    "\n",
    "@task.virtualenv(python_version=3.8)\n",
    "def materialize_holiday_task(state_dict: dict)-> dict:\n",
    "    from dags.snowpark_connection import snowpark_connect\n",
    "    from dags.mlops_pipeline import materialize_holiday_table\n",
    "\n",
    "    print('Materializing holiday table.')\n",
    "    session, _ = snowpark_connect()\n",
    "\n",
    "    _ = materialize_holiday_table(session=session, \n",
    "                                  holiday_table_name=state_dict['holiday_table_name'])\n",
    "\n",
    "    session.close()\n",
    "    return state_dict\n",
    "\n",
    "@task.virtualenv(python_version=3.8)\n",
    "def subscribe_to_weather_data_task(state_dict: dict)-> dict:\n",
    "    from dags.snowpark_connection import snowpark_connect\n",
    "    from dags.mlops_pipeline import subscribe_to_weather_data\n",
    "\n",
    "    print('Subscribing to weather data')\n",
    "    session, _ = snowpark_connect()\n",
    "\n",
    "    _ = subscribe_to_weather_data(session=session, \n",
    "                                  weather_database_name=state_dict['weather_database_name'], \n",
    "                                  weather_listing_id=state_dict['weather_listing_id'])\n",
    "    session.close()\n",
    "    return state_dict\n",
    "\n",
    "@task.virtualenv(python_version=3.8)\n",
    "def create_weather_view_task(state_dict: dict)-> dict:\n",
    "    from dags.snowpark_connection import snowpark_connect\n",
    "    from dags.mlops_pipeline import create_weather_view\n",
    "\n",
    "    print('Creating weather view')\n",
    "    session, _ = snowpark_connect()\n",
    "\n",
    "    _ = create_weather_view(session=session,\n",
    "                            weather_table_name=state_dict['weather_table_name'],\n",
    "                            weather_view_name=state_dict['weather_view_name'])\n",
    "    session.close()\n",
    "    return state_dict\n",
    "    \n",
    "@task.virtualenv(python_version=3.8)\n",
    "def deploy_model_udf_task(state_dict:dict)-> dict:\n",
    "    from dags.snowpark_connection import snowpark_connect\n",
    "    from dags.mlops_pipeline import deploy_pred_train_udf\n",
    "\n",
    "    print('Deploying station model')\n",
    "    session, _ = snowpark_connect()\n",
    "\n",
    "    _ = session.sql('CREATE STAGE IF NOT EXISTS ' + state_dict['model_stage_name']).collect()\n",
    "\n",
    "    _ = deploy_pred_train_udf(session=session, \n",
    "                              udf_name=state_dict['train_udf_name'],\n",
    "                              function_name=state_dict['train_func_name'],\n",
    "                              model_stage_name=state_dict['model_stage_name'])\n",
    "    session.close()\n",
    "    return state_dict\n",
    "\n",
    "@task.virtualenv(python_version=3.8)\n",
    "def deploy_eval_udf_task(state_dict:dict)-> dict:\n",
    "    from dags.snowpark_connection import snowpark_connect\n",
    "    from dags.mlops_pipeline import deploy_eval_udf\n",
    "\n",
    "    print('Deploying station model')\n",
    "    session, _ = snowpark_connect()\n",
    "\n",
    "    _ = session.sql('CREATE STAGE IF NOT EXISTS ' + state_dict['model_stage_name']).collect()\n",
    "\n",
    "    _ = deploy_eval_udf(session=session, \n",
    "                        udf_name=state_dict['eval_udf_name'],\n",
    "                        function_name=state_dict['eval_func_name'],\n",
    "                        model_stage_name=state_dict['model_stage_name'])\n",
    "    session.close()\n",
    "    return state_dict\n",
    "\n",
    "@task.virtualenv(python_version=3.8)\n",
    "def generate_feature_table_task(state_dict:dict, \n",
    "                                holiday_state_dict:dict, \n",
    "                                weather_state_dict:dict)-> dict:\n",
    "    from dags.snowpark_connection import snowpark_connect\n",
    "    from dags.mlops_pipeline import create_feature_table\n",
    "\n",
    "    print('Generating features for all stations.')\n",
    "    session, _ = snowpark_connect()\n",
    "\n",
    "    session.use_warehouse(state_dict['compute_parameters']['fe_warehouse'])\n",
    "\n",
    "    _ = session.sql(\"CREATE OR REPLACE TABLE \"+state_dict['clone_table_name']+\\\n",
    "                    \" CLONE \"+state_dict['trips_table_name']).collect()\n",
    "    _ = session.sql(\"ALTER TABLE \"+state_dict['clone_table_name']+\\\n",
    "                    \" SET TAG model_id_tag = '\"+state_dict['model_id']+\"'\").collect()\n",
    "\n",
    "    _ = create_feature_table(session, \n",
    "                             trips_table_name=state_dict['clone_table_name'], \n",
    "                             holiday_table_name=state_dict['holiday_table_name'], \n",
    "                             weather_view_name=state_dict['weather_view_name'],\n",
    "                             feature_table_name=state_dict['feature_table_name'])\n",
    "\n",
    "    _ = session.sql(\"ALTER TABLE \"+state_dict['feature_table_name']+\\\n",
    "                    \" SET TAG model_id_tag = '\"+state_dict['model_id']+\"'\").collect()\n",
    "\n",
    "    session.close()\n",
    "    return state_dict\n",
    "\n",
    "@task.virtualenv(python_version=3.8)\n",
    "def generate_forecast_table_task(state_dict:dict, \n",
    "                                 holiday_state_dict:dict, \n",
    "                                 weather_state_dict:dict)-> dict: \n",
    "    from dags.snowpark_connection import snowpark_connect\n",
    "    from dags.mlops_pipeline import create_forecast_table\n",
    "\n",
    "    print('Generating forecast features.')\n",
    "    session, _ = snowpark_connect()\n",
    "\n",
    "    _ = create_forecast_table(session, \n",
    "                              trips_table_name=state_dict['trips_table_name'],\n",
    "                              holiday_table_name=state_dict['holiday_table_name'], \n",
    "                              weather_view_name=state_dict['weather_view_name'], \n",
    "                              forecast_table_name=state_dict['forecast_table_name'],\n",
    "                              steps=state_dict['forecast_steps'])\n",
    "\n",
    "    _ = session.sql(\"ALTER TABLE \"+state_dict['forecast_table_name']+\\\n",
    "                    \" SET TAG model_id_tag = '\"+state_dict['model_id']+\"'\").collect()\n",
    "\n",
    "    session.close()\n",
    "    return state_dict\n",
    "\n",
    "@task.virtualenv(python_version=3.8)\n",
    "def bulk_train_predict_task(state_dict:dict, \n",
    "                            feature_state_dict:dict, \n",
    "                            forecast_state_dict:dict)-> dict: \n",
    "    from dags.snowpark_connection import snowpark_connect\n",
    "    from dags.mlops_pipeline import train_predict\n",
    "\n",
    "    state_dict = feature_state_dict\n",
    "\n",
    "    print('Running bulk training and forecast.')\n",
    "    session, _ = snowpark_connect()\n",
    "\n",
    "    session.use_warehouse(state_dict['compute_parameters']['train_warehouse'])\n",
    "\n",
    "    pred_table_name = train_predict(session, \n",
    "                                    station_train_pred_udf_name=state_dict['train_udf_name'], \n",
    "                                    feature_table_name=state_dict['feature_table_name'], \n",
    "                                    forecast_table_name=state_dict['forecast_table_name'],\n",
    "                                    pred_table_name=state_dict['pred_table_name'])\n",
    "\n",
    "    _ = session.sql(\"ALTER TABLE \"+state_dict['pred_table_name']+\\\n",
    "                    \" SET TAG model_id_tag = '\"+state_dict['model_id']+\"'\").collect()\n",
    "    #_ = session.sql('ALTER WAREHOUSE IF EXISTS '+state_dict['compute_parameters']['train_warehouse']+\\\n",
    "    #                ' SUSPEND').collect()\n",
    "\n",
    "    session.close()\n",
    "    return state_dict\n",
    "\n",
    "@task.virtualenv(python_version=3.8)\n",
    "def eval_station_models_task(state_dict:dict, \n",
    "                             pred_state_dict:dict,\n",
    "                             run_date:str)-> dict:\n",
    "\n",
    "    from dags.snowpark_connection import snowpark_connect\n",
    "    from dags.mlops_pipeline import evaluate_station_model\n",
    "\n",
    "    print('Running eval UDF for model output')\n",
    "    session, _ = snowpark_connect()\n",
    "\n",
    "    eval_table_name = evaluate_station_model(session, \n",
    "                                             run_date=run_date, \n",
    "                                             eval_model_udf_name=state_dict['eval_udf_name'], \n",
    "                                             pred_table_name=state_dict['pred_table_name'], \n",
    "                                             eval_table_name=state_dict['eval_table_name'])\n",
    "\n",
    "    _ = session.sql(\"ALTER TABLE \"+state_dict['eval_table_name']+\\\n",
    "                    \" SET TAG model_id_tag = '\"+state_dict['model_id']+\"'\").collect()\n",
    "    session.close()\n",
    "    return state_dict                                               \n",
    "\n",
    "@task.virtualenv(python_version=3.8)\n",
    "def flatten_tables_task(pred_state_dict:dict, state_dict:dict)-> dict:\n",
    "    from dags.snowpark_connection import snowpark_connect\n",
    "    from dags.mlops_pipeline import flatten_tables\n",
    "\n",
    "    print('Flattening tables for end-user consumption.')\n",
    "    session, _ = snowpark_connect()\n",
    "\n",
    "    flat_pred_table, flat_forecast_table, flat_eval_table = flatten_tables(session,\n",
    "                                                                           pred_table_name=state_dict['pred_table_name'], \n",
    "                                                                           forecast_table_name=state_dict['forecast_table_name'], \n",
    "                                                                           eval_table_name=state_dict['eval_table_name'])\n",
    "    state_dict['flat_pred_table'] = flat_pred_table\n",
    "    state_dict['flat_forecast_table'] = flat_forecast_table\n",
    "    state_dict['flat_eval_table'] = flat_eval_table\n",
    "\n",
    "    _ = session.sql(\"ALTER TABLE \"+flat_pred_table+\" SET TAG model_id_tag = '\"+state_dict['model_id']+\"'\").collect()\n",
    "    _ = session.sql(\"ALTER TABLE \"+flat_forecast_table+\" SET TAG model_id_tag = '\"+state_dict['model_id']+\"'\").collect()\n",
    "    _ = session.sql(\"ALTER TABLE \"+flat_eval_table+\" SET TAG model_id_tag = '\"+state_dict['model_id']+\"'\").collect()\n",
    "\n",
    "    return state_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5a42735e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting dags/airflow_setup_pipeline.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile dags/airflow_setup_pipeline.py\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "from airflow.decorators import dag, task\n",
    "from dags.airflow_tasks import snowpark_database_setup\n",
    "from dags.airflow_tasks import incremental_elt_task\n",
    "from dags.airflow_tasks import initial_bulk_load_task\n",
    "from dags.airflow_tasks import materialize_holiday_task\n",
    "from dags.airflow_tasks import subscribe_to_weather_data_task\n",
    "from dags.airflow_tasks import create_weather_view_task\n",
    "from dags.airflow_tasks import deploy_model_udf_task\n",
    "from dags.airflow_tasks import deploy_eval_udf_task\n",
    "from dags.airflow_tasks import generate_feature_table_task\n",
    "from dags.airflow_tasks import generate_forecast_table_task\n",
    "from dags.airflow_tasks import bulk_train_predict_task\n",
    "from dags.airflow_tasks import eval_station_models_task \n",
    "from dags.airflow_tasks import flatten_tables_task\n",
    "\n",
    "default_args = {\n",
    "    'owner': 'airflow',\n",
    "    'depends_on_past': False,\n",
    "    'email_on_failure': False,\n",
    "    'email_on_retry': False,\n",
    "    'retries': 1,\n",
    "    'retry_delay': timedelta(minutes=5)\n",
    "}\n",
    "\n",
    "#local_airflow_path = '/usr/local/airflow/'\n",
    "\n",
    "@dag(default_args=default_args, schedule_interval=None, start_date=datetime(2020, 3, 1), catchup=False, tags=['setup'])\n",
    "def citibikeml_setup_taskflow(run_date:str):\n",
    "    \"\"\"\n",
    "    Setup initial Snowpark / Astronomer ML Demo\n",
    "    \"\"\"\n",
    "    import uuid\n",
    "    import json\n",
    "    \n",
    "    with open('./include/state.json') as sdf:\n",
    "        state_dict = json.load(sdf)\n",
    "    \n",
    "    model_id = str(uuid.uuid1()).replace('-', '_')\n",
    "\n",
    "    state_dict.update({'model_id': model_id})\n",
    "    state_dict.update({'run_date': run_date})\n",
    "    state_dict.update({'weather_database_name': 'WEATHER_NYC'})\n",
    "    state_dict.update({'load_table_name': 'RAW_',\n",
    "                       'trips_table_name': 'TRIPS',\n",
    "                       'load_stage_name': 'LOAD_STAGE',\n",
    "                       'model_stage_name': 'MODEL_STAGE',\n",
    "                       'weather_table_name': state_dict['weather_database_name']+'.ONPOINT_ID.HISTORY_DAY',\n",
    "                       'weather_view_name': 'WEATHER_NYC_VW',\n",
    "                       'holiday_table_name': 'HOLIDAYS',\n",
    "                       'clone_table_name': 'CLONE_'+model_id,\n",
    "                       'feature_table_name' : 'FEATURE_'+model_id,\n",
    "                       'pred_table_name': 'PRED_'+model_id,\n",
    "                       'eval_table_name': 'EVAL_'+model_id,\n",
    "                       'forecast_table_name': 'FORECAST_'+model_id,\n",
    "                       'forecast_steps': 30,\n",
    "                       'train_udf_name': 'station_train_predict_udf',\n",
    "                       'train_func_name': 'station_train_predict_func',\n",
    "                       'eval_udf_name': 'eval_model_output_udf',\n",
    "                       'eval_func_name': 'eval_model_func'\n",
    "                      })\n",
    "    \n",
    "    #Task order - one-time setup\n",
    "    setup_state_dict = snowpark_database_setup(state_dict)\n",
    "    load_state_dict = initial_bulk_load_task(setup_state_dict)\n",
    "    holiday_state_dict = materialize_holiday_task(setup_state_dict)\n",
    "    subscribe_state_dict = subscribe_to_weather_data_task(setup_state_dict)\n",
    "    weather_state_dict = create_weather_view_task(subscribe_state_dict)\n",
    "    model_udf_state_dict = deploy_model_udf_task(setup_state_dict)\n",
    "    eval_udf_state_dict = deploy_eval_udf_task(setup_state_dict)\n",
    "    feature_state_dict = generate_feature_table_task(load_state_dict, holiday_state_dict, weather_state_dict) \n",
    "    foecast_state_dict = generate_forecast_table_task(load_state_dict, holiday_state_dict, weather_state_dict)\n",
    "    pred_state_dict = bulk_train_predict_task(model_udf_state_dict, feature_state_dict, foecast_state_dict)\n",
    "    eval_state_dict = eval_station_models_task(eval_udf_state_dict, pred_state_dict, run_date)  \n",
    "    state_dict = flatten_tables_task(pred_state_dict, eval_state_dict)\n",
    "\n",
    "    return state_dict\n",
    "\n",
    "run_date='2020_01_01'\n",
    "\n",
    "state_dict = citibikeml_setup_taskflow(run_date=run_date)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "29fdd741",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting dags/airflow_incremental_pipeline.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile dags/airflow_incremental_pipeline.py\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "from airflow.decorators import dag, task\n",
    "from dags.airflow_tasks import snowpark_database_setup\n",
    "from dags.airflow_tasks import incremental_elt_task\n",
    "from dags.airflow_tasks import initial_bulk_load_task\n",
    "from dags.airflow_tasks import materialize_holiday_task\n",
    "from dags.airflow_tasks import deploy_model_udf_task\n",
    "from dags.airflow_tasks import deploy_eval_udf_task\n",
    "from dags.airflow_tasks import generate_feature_table_task\n",
    "from dags.airflow_tasks import generate_forecast_table_task\n",
    "from dags.airflow_tasks import bulk_train_predict_task\n",
    "from dags.airflow_tasks import eval_station_models_task \n",
    "from dags.airflow_tasks import flatten_tables_task\n",
    "\n",
    "default_args = {\n",
    "    'owner': 'airflow',\n",
    "    'depends_on_past': False,\n",
    "    'email_on_failure': False,\n",
    "    'email_on_retry': False,\n",
    "    'retries': 1,\n",
    "    'retry_delay': timedelta(minutes=5)\n",
    "}\n",
    "\n",
    "#local_airflow_path = '/usr/local/airflow/'\n",
    "\n",
    "@dag(default_args=default_args, schedule_interval=None, start_date=datetime(2020, 4, 1), catchup=False, tags=['monthly'])\n",
    "def citibikeml_monthly_taskflow(files_to_download:list, run_date:str):\n",
    "    \"\"\"\n",
    "    End to end Snowpark / Astronomer ML Demo\n",
    "    \"\"\"\n",
    "    import uuid\n",
    "    import json\n",
    "    \n",
    "    with open('./include/state.json') as sdf:\n",
    "        state_dict = json.load(sdf)\n",
    "    \n",
    "    model_id = str(uuid.uuid1()).replace('-', '_')\n",
    "\n",
    "    state_dict.update({'model_id': model_id})\n",
    "    state_dict.update({'run_date': run_date})\n",
    "    state_dict.update({'weather_database_name': 'WEATHER_NYC'})\n",
    "    state_dict.update({'load_table_name': 'RAW_',\n",
    "                       'trips_table_name': 'TRIPS',\n",
    "                       'load_stage_name': 'LOAD_STAGE',\n",
    "                       'model_stage_name': 'MODEL_STAGE',\n",
    "                       'weather_table_name': state_dict['weather_database_name']+'.ONPOINT_ID.HISTORY_DAY',\n",
    "                       'weather_view_name': 'WEATHER_NYC_VW',\n",
    "                       'holiday_table_name': 'HOLIDAYS',\n",
    "                       'clone_table_name': 'CLONE_'+model_id,\n",
    "                       'feature_table_name' : 'FEATURE_'+model_id,\n",
    "                       'pred_table_name': 'PRED_'+model_id,\n",
    "                       'eval_table_name': 'EVAL_'+model_id,\n",
    "                       'forecast_table_name': 'FORECAST_'+model_id,\n",
    "                       'forecast_steps': 30,\n",
    "                       'train_udf_name': 'station_train_predict_udf',\n",
    "                       'train_func_name': 'station_train_predict_func',\n",
    "                       'eval_udf_name': 'eval_model_output_udf',\n",
    "                       'eval_func_name': 'eval_model_func'\n",
    "                      })\n",
    "\n",
    "    incr_state_dict = incremental_elt_task(state_dict, files_to_download)\n",
    "    feature_state_dict = generate_feature_table_task(incr_state_dict, incr_state_dict, incr_state_dict) \n",
    "    forecast_state_dict = generate_forecast_table_task(incr_state_dict, incr_state_dict, incr_state_dict)\n",
    "    pred_state_dict = bulk_train_predict_task(feature_state_dict, feature_state_dict, forecast_state_dict)\n",
    "    eval_state_dict = eval_station_models_task(pred_state_dict, pred_state_dict, run_date)\n",
    "    state_dict = flatten_tables_task(pred_state_dict, eval_state_dict)\n",
    "\n",
    "    return state_dict\n",
    "\n",
    "run_date='2020_02_01'\n",
    "files_to_download = ['202001-citibike-tripdata.csv.zip']\n",
    "\n",
    "state_dict = citibikeml_monthly_taskflow(files_to_download=files_to_download, \n",
    "                                         run_date=run_date)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b183b8fa",
   "metadata": {},
   "source": [
    "Now open a new browser tab to localhost:8080"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5961a58b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import webbrowser\n",
    "\n",
    "# generate an URL\n",
    "url = 'http://localhost:8080'\n",
    "webbrowser.open(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd1ebc5",
   "metadata": {},
   "source": [
    "Lets run the initial setup, ingest and forecast DAG."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4f1e1041",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #This sample code can be used to trigger the Airflow pipeline from a command-line shell.\n",
    "# !curl -X POST 'http://localhost:8080/api/v1/dags/citibikeml_monthly_taskflow/dagRuns' \\\n",
    "# -H 'Content-Type: application/json' \\\n",
    "# --user \"admin:admin\" \\\n",
    "# -d '{\"conf\": {\"files_to_download\": [\"202003-citibike-tripdata.csv.zip\"], \"run_date\": \"2020_04_01\"}}'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb0d7c97",
   "metadata": {},
   "source": [
    "Alternatively we can use a REST API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b84288d7",
   "metadata": {},
   "outputs": [
    {
     "ename": "ConnectionError",
     "evalue": "HTTPConnectionPool(host='localhost', port=8080): Max retries exceeded with url: /api/v1/dags/citibikeml_setup_taskflow/dagRuns (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f353ae52b20>: Failed to establish a new connection: [Errno 111] Connection refused'))",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mConnectionRefusedError\u001b[0m                    Traceback (most recent call last)",
      "File \u001b[0;32m~/.conda/envs/snowpark_0110/lib/python3.8/site-packages/urllib3/connection.py:174\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 174\u001b[0m     conn \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_connection\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dns_host\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mport\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mextra_kw\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m SocketTimeout:\n",
      "File \u001b[0;32m~/.conda/envs/snowpark_0110/lib/python3.8/site-packages/urllib3/util/connection.py:95\u001b[0m, in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 95\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m err\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m socket\u001b[38;5;241m.\u001b[39merror(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgetaddrinfo returns an empty list\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.conda/envs/snowpark_0110/lib/python3.8/site-packages/urllib3/util/connection.py:85\u001b[0m, in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[1;32m     84\u001b[0m     sock\u001b[38;5;241m.\u001b[39mbind(source_address)\n\u001b[0;32m---> 85\u001b[0m \u001b[43msock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43msa\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m sock\n",
      "\u001b[0;31mConnectionRefusedError\u001b[0m: [Errno 111] Connection refused",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mNewConnectionError\u001b[0m                        Traceback (most recent call last)",
      "File \u001b[0;32m~/.conda/envs/snowpark_0110/lib/python3.8/site-packages/urllib3/connectionpool.py:703\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    702\u001b[0m \u001b[38;5;66;03m# Make the request on the httplib connection object.\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    704\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    705\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    706\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    707\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    708\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    709\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    710\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    711\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    713\u001b[0m \u001b[38;5;66;03m# If we're going to release the connection in ``finally:``, then\u001b[39;00m\n\u001b[1;32m    714\u001b[0m \u001b[38;5;66;03m# the response doesn't need to know about the connection. Otherwise\u001b[39;00m\n\u001b[1;32m    715\u001b[0m \u001b[38;5;66;03m# it will also try to release it and we'll have a double-release\u001b[39;00m\n\u001b[1;32m    716\u001b[0m \u001b[38;5;66;03m# mess.\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/snowpark_0110/lib/python3.8/site-packages/urllib3/connectionpool.py:398\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    397\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 398\u001b[0m         \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhttplib_request_kw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    400\u001b[0m \u001b[38;5;66;03m# We are swallowing BrokenPipeError (errno.EPIPE) since the server is\u001b[39;00m\n\u001b[1;32m    401\u001b[0m \u001b[38;5;66;03m# legitimately able to close the connection after sending a valid response.\u001b[39;00m\n\u001b[1;32m    402\u001b[0m \u001b[38;5;66;03m# With this behaviour, the received response is still readable.\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/snowpark_0110/lib/python3.8/site-packages/urllib3/connection.py:239\u001b[0m, in \u001b[0;36mHTTPConnection.request\u001b[0;34m(self, method, url, body, headers)\u001b[0m\n\u001b[1;32m    238\u001b[0m     headers[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUser-Agent\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m _get_default_user_agent()\n\u001b[0;32m--> 239\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mHTTPConnection\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/snowpark_0110/lib/python3.8/http/client.py:1256\u001b[0m, in \u001b[0;36mHTTPConnection.request\u001b[0;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[1;32m   1255\u001b[0m \u001b[38;5;124;03m\"\"\"Send a complete request to the server.\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1256\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencode_chunked\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/snowpark_0110/lib/python3.8/http/client.py:1302\u001b[0m, in \u001b[0;36mHTTPConnection._send_request\u001b[0;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[1;32m   1301\u001b[0m     body \u001b[38;5;241m=\u001b[39m _encode(body, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbody\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m-> 1302\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mendheaders\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencode_chunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencode_chunked\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/snowpark_0110/lib/python3.8/http/client.py:1251\u001b[0m, in \u001b[0;36mHTTPConnection.endheaders\u001b[0;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[1;32m   1250\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CannotSendHeader()\n\u001b[0;32m-> 1251\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_output\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessage_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencode_chunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencode_chunked\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/snowpark_0110/lib/python3.8/http/client.py:1011\u001b[0m, in \u001b[0;36mHTTPConnection._send_output\u001b[0;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[1;32m   1010\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_buffer[:]\n\u001b[0;32m-> 1011\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1013\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m message_body \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1014\u001b[0m \n\u001b[1;32m   1015\u001b[0m     \u001b[38;5;66;03m# create a consistent interface to message_body\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/snowpark_0110/lib/python3.8/http/client.py:951\u001b[0m, in \u001b[0;36mHTTPConnection.send\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    950\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_open:\n\u001b[0;32m--> 951\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    952\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/.conda/envs/snowpark_0110/lib/python3.8/site-packages/urllib3/connection.py:205\u001b[0m, in \u001b[0;36mHTTPConnection.connect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconnect\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 205\u001b[0m     conn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_new_conn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    206\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_conn(conn)\n",
      "File \u001b[0;32m~/.conda/envs/snowpark_0110/lib/python3.8/site-packages/urllib3/connection.py:186\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    185\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m SocketError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 186\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m NewConnectionError(\n\u001b[1;32m    187\u001b[0m         \u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to establish a new connection: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m e\n\u001b[1;32m    188\u001b[0m     )\n\u001b[1;32m    190\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m conn\n",
      "\u001b[0;31mNewConnectionError\u001b[0m: <urllib3.connection.HTTPConnection object at 0x7f353ae52b20>: Failed to establish a new connection: [Errno 111] Connection refused",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mMaxRetryError\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[0;32m~/.conda/envs/snowpark_0110/lib/python3.8/site-packages/requests/adapters.py:489\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    488\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m chunked:\n\u001b[0;32m--> 489\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    491\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    492\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    495\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    496\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    497\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    499\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    502\u001b[0m \u001b[38;5;66;03m# Send the request.\u001b[39;00m\n\u001b[1;32m    503\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/.conda/envs/snowpark_0110/lib/python3.8/site-packages/urllib3/connectionpool.py:787\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    785\u001b[0m     e \u001b[38;5;241m=\u001b[39m ProtocolError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConnection aborted.\u001b[39m\u001b[38;5;124m\"\u001b[39m, e)\n\u001b[0;32m--> 787\u001b[0m retries \u001b[38;5;241m=\u001b[39m \u001b[43mretries\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mincrement\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    788\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_pool\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_stacktrace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexc_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m    789\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    790\u001b[0m retries\u001b[38;5;241m.\u001b[39msleep()\n",
      "File \u001b[0;32m~/.conda/envs/snowpark_0110/lib/python3.8/site-packages/urllib3/util/retry.py:592\u001b[0m, in \u001b[0;36mRetry.increment\u001b[0;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m new_retry\u001b[38;5;241m.\u001b[39mis_exhausted():\n\u001b[0;32m--> 592\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m MaxRetryError(_pool, url, error \u001b[38;5;129;01mor\u001b[39;00m ResponseError(cause))\n\u001b[1;32m    594\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIncremented Retry for (url=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m): \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, url, new_retry)\n",
      "\u001b[0;31mMaxRetryError\u001b[0m: HTTPConnectionPool(host='localhost', port=8080): Max retries exceeded with url: /api/v1/dags/citibikeml_setup_taskflow/dagRuns (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f353ae52b20>: Failed to establish a new connection: [Errno 111] Connection refused'))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mConnectionError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [15], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m dag_url\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttp://localhost:8080/api/v1/dags/citibikeml_setup_taskflow/dagRuns\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      7\u001b[0m json_payload \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconf\u001b[39m\u001b[38;5;124m\"\u001b[39m: {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_date\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2020_01_01\u001b[39m\u001b[38;5;124m\"\u001b[39m}}\n\u001b[0;32m----> 9\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mrequests\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpost\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdag_url\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjson_payload\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mauth\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mHTTPBasicAuth\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43madmin\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43madmin\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m run_id \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mloads(response\u001b[38;5;241m.\u001b[39mtext)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdag_run_id\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     15\u001b[0m state\u001b[38;5;241m=\u001b[39mjson\u001b[38;5;241m.\u001b[39mloads(requests\u001b[38;5;241m.\u001b[39mget(dag_url\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39mrun_id, auth\u001b[38;5;241m=\u001b[39mHTTPBasicAuth(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madmin\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124madmin\u001b[39m\u001b[38;5;124m'\u001b[39m))\u001b[38;5;241m.\u001b[39mtext)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstate\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[0;32m~/.conda/envs/snowpark_0110/lib/python3.8/site-packages/requests/api.py:115\u001b[0m, in \u001b[0;36mpost\u001b[0;34m(url, data, json, **kwargs)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(url, data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, json\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    104\u001b[0m     \u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a POST request.\u001b[39;00m\n\u001b[1;32m    105\u001b[0m \n\u001b[1;32m    106\u001b[0m \u001b[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;124;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 115\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpost\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjson\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/snowpark_0110/lib/python3.8/site-packages/requests/api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sessions\u001b[38;5;241m.\u001b[39mSession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[0;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/snowpark_0110/lib/python3.8/site-packages/requests/sessions.py:587\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    582\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    583\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[1;32m    584\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    585\u001b[0m }\n\u001b[1;32m    586\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 587\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    589\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m~/.conda/envs/snowpark_0110/lib/python3.8/site-packages/requests/sessions.py:701\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    698\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[1;32m    700\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[0;32m--> 701\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43madapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    704\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[0;32m~/.conda/envs/snowpark_0110/lib/python3.8/site-packages/requests/adapters.py:565\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    561\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e\u001b[38;5;241m.\u001b[39mreason, _SSLError):\n\u001b[1;32m    562\u001b[0m         \u001b[38;5;66;03m# This branch is for urllib3 v1.22 and later.\u001b[39;00m\n\u001b[1;32m    563\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m SSLError(e, request\u001b[38;5;241m=\u001b[39mrequest)\n\u001b[0;32m--> 565\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(e, request\u001b[38;5;241m=\u001b[39mrequest)\n\u001b[1;32m    567\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ClosedPoolError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    568\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(e, request\u001b[38;5;241m=\u001b[39mrequest)\n",
      "\u001b[0;31mConnectionError\u001b[0m: HTTPConnectionPool(host='localhost', port=8080): Max retries exceeded with url: /api/v1/dags/citibikeml_setup_taskflow/dagRuns (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f353ae52b20>: Failed to establish a new connection: [Errno 111] Connection refused'))"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from requests.auth import HTTPBasicAuth\n",
    "import time \n",
    "import json\n",
    "\n",
    "dag_url='http://localhost:8080/api/v1/dags/citibikeml_setup_taskflow/dagRuns'\n",
    "json_payload = {\"conf\": {\"run_date\": \"2020_01_01\"}}\n",
    "\n",
    "response = requests.post(dag_url, \n",
    "                        json=json_payload,\n",
    "                        auth = HTTPBasicAuth('admin', 'admin'))\n",
    "\n",
    "run_id = json.loads(response.text)['dag_run_id']\n",
    "\n",
    "state=json.loads(requests.get(dag_url+'/'+run_id, auth=HTTPBasicAuth('admin', 'admin')).text)['state']\n",
    "\n",
    "while state != 'success':\n",
    "    print('DAG running...'+state)\n",
    "    time.sleep(10)\n",
    "    state=json.loads(requests.get(dag_url+'/'+run_id, auth=HTTPBasicAuth('admin', 'admin')).text)['state']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f3dbec",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from requests.auth import HTTPBasicAuth\n",
    "import time \n",
    "import json\n",
    "\n",
    "dag_url='http://localhost:8080/api/v1/dags/citibikeml_monthly_taskflow/dagRuns'\n",
    "json_payload = {\"conf\": {\"files_to_download\": [\"202001-citibike-tripdata.csv.zip\"], \"run_date\": \"2020_02_01\"}}\n",
    "\n",
    "response = requests.post(dag_url, \n",
    "                        json=json_payload,\n",
    "                        auth = HTTPBasicAuth('admin', 'admin'))\n",
    "\n",
    "run_id = json.loads(response.text)['dag_run_id']\n",
    "\n",
    "state=json.loads(requests.get(dag_url+'/'+run_id, auth=HTTPBasicAuth('admin', 'admin')).text)['state']\n",
    "\n",
    "while state != 'success':\n",
    "    print('DAG running...'+state)\n",
    "    time.sleep(10)\n",
    "    state=json.loads(requests.get(dag_url+'/'+run_id, auth=HTTPBasicAuth('admin', 'admin')).text)['state']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad097d86",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "snowpark_0110:Python",
   "language": "python",
   "name": "conda-env-snowpark_0110-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
